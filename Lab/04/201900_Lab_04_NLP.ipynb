{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30028,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks with Keras\n",
        "\n",
        "## Sentiment Analysis from Movie Reviews\n",
        "\n",
        "This notebook draws inspiration from the imdb_lstm.py example included with Keras.\n",
        "\n",
        "It serves as an excellent illustration of RNNs. The dataset utilized contains user-generated movie reviews along with classifications indicating whether the user liked the movie based on its associated rating.\n",
        "\n",
        "For more details on the dataset, refer to [this link](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification).\n",
        "\n",
        "We're employing an RNN to conduct sentiment analysis on full-text movie reviews.\n",
        "\n",
        "Consider the significance of this task. We're training an artificial neural network to comprehend movie reviews and infer whether the author enjoyed the movie.\n",
        "\n",
        "Understanding written language entails maintaining a record of all the words in a sentence. Thus, we require a recurrent neural network to retain a \"memory\" of preceding words as it processes sentences over time.\n",
        "\n",
        "Specifically, we'll utilize LSTM (Long Short-Term Memory) cells because we aim to preserve information from words encountered earlier in a sentence, as they can significantly influence its meaning.\n",
        "\n",
        "Let's begin by importing the necessary components:\n"
      ],
      "metadata": {
        "id": "poCEGt6pifg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing module of the sequential model, so we can embed those different layers toghether\n",
        "import numpy as np\n",
        "import json\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from keras.models import load_model"
      ],
      "metadata": {
        "trusted": true,
        "id": "G8RVgv_xifhE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the training and testing data, limiting to the top 20,000 most popular words for manageability.\n",
        "The dataset comprises 5,000 training reviews and 25,000 testing reviews.\n",
        "\n"
      ],
      "metadata": {
        "id": "naUiHxwzifhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# num_words: how many unique words that you want to load into your training and testing dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
        "\n",
        "# Get the word index\n",
        "word_index = imdb.get_word_index(path=\"imdb_word_index.json\")\n",
        "\n",
        "# Reverse the word index to map indices to words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwHr2D2kifhI",
        "outputId": "ee3ad997-ae9d-4e5e-f1a1-48f1571842c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, models typically operate with numerical data rather than words. Therefore, it's necessary to convert words to numbers as an initial step. Thankfully, Keras has already handled this pre-processing for us.\n"
      ],
      "metadata": {
        "id": "4J3sOC8PifhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[1337],len(x_train[1337])"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkvF33W5ifhJ",
        "outputId": "d1a7fc4f-fae3-49b4-f933-918be1e1192d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1,\n",
              "  3934,\n",
              "  1659,\n",
              "  2173,\n",
              "  9,\n",
              "  6,\n",
              "  2,\n",
              "  6282,\n",
              "  8258,\n",
              "  2930,\n",
              "  8,\n",
              "  521,\n",
              "  18,\n",
              "  2217,\n",
              "  31,\n",
              "  251,\n",
              "  190,\n",
              "  29,\n",
              "  571,\n",
              "  725,\n",
              "  35,\n",
              "  154,\n",
              "  312,\n",
              "  396,\n",
              "  19275,\n",
              "  773,\n",
              "  2,\n",
              "  2,\n",
              "  4289,\n",
              "  2065,\n",
              "  9,\n",
              "  35,\n",
              "  3604,\n",
              "  5,\n",
              "  1344,\n",
              "  1576,\n",
              "  2133,\n",
              "  19,\n",
              "  6,\n",
              "  1334,\n",
              "  393,\n",
              "  5,\n",
              "  35,\n",
              "  1724,\n",
              "  1103,\n",
              "  3934,\n",
              "  9,\n",
              "  9807,\n",
              "  29,\n",
              "  6261,\n",
              "  2,\n",
              "  8,\n",
              "  413,\n",
              "  27,\n",
              "  5294,\n",
              "  21,\n",
              "  80,\n",
              "  2,\n",
              "  28,\n",
              "  8,\n",
              "  3552,\n",
              "  41,\n",
              "  8166,\n",
              "  7,\n",
              "  4374,\n",
              "  11,\n",
              "  4,\n",
              "  182,\n",
              "  7,\n",
              "  2420,\n",
              "  121,\n",
              "  183,\n",
              "  26,\n",
              "  24,\n",
              "  210,\n",
              "  51,\n",
              "  36,\n",
              "  306,\n",
              "  8,\n",
              "  30,\n",
              "  5,\n",
              "  80,\n",
              "  59,\n",
              "  30,\n",
              "  502,\n",
              "  8,\n",
              "  15144,\n",
              "  4,\n",
              "  2,\n",
              "  7,\n",
              "  6,\n",
              "  2494,\n",
              "  1776,\n",
              "  19069,\n",
              "  14,\n",
              "  9,\n",
              "  669,\n",
              "  6,\n",
              "  327,\n",
              "  883,\n",
              "  18,\n",
              "  148,\n",
              "  37,\n",
              "  6467,\n",
              "  2953,\n",
              "  7,\n",
              "  5499,\n",
              "  3934,\n",
              "  5,\n",
              "  2,\n",
              "  26,\n",
              "  427,\n",
              "  11,\n",
              "  119,\n",
              "  5,\n",
              "  68,\n",
              "  6158,\n",
              "  5,\n",
              "  2599,\n",
              "  26,\n",
              "  6,\n",
              "  52,\n",
              "  650,\n",
              "  190,\n",
              "  261,\n",
              "  4,\n",
              "  20,\n",
              "  497,\n",
              "  8,\n",
              "  123,\n",
              "  4,\n",
              "  993,\n",
              "  113,\n",
              "  11,\n",
              "  94,\n",
              "  635,\n",
              "  12,\n",
              "  152,\n",
              "  340,\n",
              "  3115,\n",
              "  14463,\n",
              "  4,\n",
              "  365,\n",
              "  1234,\n",
              "  26,\n",
              "  312,\n",
              "  5,\n",
              "  4,\n",
              "  229,\n",
              "  9,\n",
              "  55,\n",
              "  14540,\n",
              "  398,\n",
              "  19,\n",
              "  134,\n",
              "  14618,\n",
              "  5,\n",
              "  4,\n",
              "  2252,\n",
              "  2,\n",
              "  7,\n",
              "  2065,\n",
              "  5,\n",
              "  2173,\n",
              "  148,\n",
              "  37,\n",
              "  870,\n",
              "  180,\n",
              "  8,\n",
              "  4,\n",
              "  22,\n",
              "  80,\n",
              "  169,\n",
              "  12,\n",
              "  8,\n",
              "  30,\n",
              "  52,\n",
              "  722],\n",
              " 178)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels in our dataset represent binary sentiment classifications, where 0 indicates a negative sentiment (the reviewer did not like the movie) and 1 indicates a positive sentiment (the reviewer liked the movie). These labels serve as the target variable for our sentiment analysis task."
      ],
      "metadata": {
        "id": "g9XhFumLifhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[1337]"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyayxZb3ifhK",
        "outputId": "73c0f66d-ce97-4243-be7d-f9641e86a4fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, we're dealing with movie reviews represented as sequences of integers and corresponding binary sentiment classifications (0 or 1), indicating whether the reviewer liked the movie.\n",
        "\n",
        "Given the potential computational complexity of RNNs, we'll constrain our analysis to the first 80 words of each review to maintain manageability on our PC.\n"
      ],
      "metadata": {
        "id": "IGloT1nzifhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = sequence.pad_sequences(x_train, maxlen=80)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=80)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7WUIkz3jifhM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up a neural network model with Keras is surprisingly simple, even for a complicated architecture like LSTM recurrent neural networks.\n",
        "\n",
        "We begin with an Embedding layer, which converts input data into dense vectors of fixed size. This is particularly useful for index-based text data like ours. We specify a vocabulary size of 20,000 and an output dimension of 128 units.\n",
        "\n",
        "Next, we add an LSTM layer for the recurrent neural network itself. We set the output size to 128 to match the output of the Embedding layer, and include dropout terms to prevent overfitting, a common issue with RNNs.\n",
        "\n",
        "Finally, we reduce it to a single neuron with a sigmoid activation function for binary sentiment classification (0 or 1).\n"
      ],
      "metadata": {
        "id": "EJtgIbIsifhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(20000, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR8aPzCxifhN",
        "outputId": "3ef26c77-b2f2-41a6-8f85-faa8a57493c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this is a binary classification problem, we'll employ the binary_crossentropy loss function. The Adam optimizer is typically a good choice (although you can experiment with others).\n"
      ],
      "metadata": {
        "id": "z10DnwZ2ifhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "JdSyjXHQifhO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to train our model. RNNs, like CNNs, are resource-intensive. Maintaining a small batch size is crucial for running this on your PC. To scale better, leverage GPUs across computers in a cluster.\n",
        "\n",
        "## Warning\n",
        "\n",
        "Executing the next blocks will take a long time, even on a fast PC! Proceed only if you're willing to tie up your computer for an hour or more.\n"
      ],
      "metadata": {
        "id": "ImS3hzucifhO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's kick off the training. Even with a GPU, this will take a long time!"
      ],
      "metadata": {
        "id": "Hlf1S7gZifhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=15,\n",
        "          verbose=2,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOWrvS9gifhO",
        "outputId": "9f2d8017-050b-49f1-b04e-553867f1f49a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "782/782 - 230s - loss: 0.4232 - accuracy: 0.8014 - val_loss: 0.3863 - val_accuracy: 0.8368 - 230s/epoch - 294ms/step\n",
            "Epoch 2/15\n",
            "782/782 - 212s - loss: 0.2524 - accuracy: 0.8991 - val_loss: 0.3737 - val_accuracy: 0.8365 - 212s/epoch - 271ms/step\n",
            "Epoch 3/15\n",
            "782/782 - 208s - loss: 0.1675 - accuracy: 0.9380 - val_loss: 0.5574 - val_accuracy: 0.8138 - 208s/epoch - 265ms/step\n",
            "Epoch 4/15\n",
            "782/782 - 207s - loss: 0.1089 - accuracy: 0.9610 - val_loss: 0.6462 - val_accuracy: 0.8138 - 207s/epoch - 264ms/step\n",
            "Epoch 5/15\n",
            "782/782 - 211s - loss: 0.0756 - accuracy: 0.9741 - val_loss: 0.6275 - val_accuracy: 0.8166 - 211s/epoch - 270ms/step\n",
            "Epoch 6/15\n",
            "782/782 - 211s - loss: 0.0514 - accuracy: 0.9832 - val_loss: 0.6423 - val_accuracy: 0.8027 - 211s/epoch - 270ms/step\n",
            "Epoch 7/15\n",
            "782/782 - 208s - loss: 0.0460 - accuracy: 0.9849 - val_loss: 0.7061 - val_accuracy: 0.8141 - 208s/epoch - 266ms/step\n",
            "Epoch 8/15\n",
            "782/782 - 208s - loss: 0.0244 - accuracy: 0.9924 - val_loss: 1.0513 - val_accuracy: 0.8134 - 208s/epoch - 267ms/step\n",
            "Epoch 9/15\n",
            "782/782 - 207s - loss: 0.0220 - accuracy: 0.9932 - val_loss: 0.8941 - val_accuracy: 0.8020 - 207s/epoch - 264ms/step\n",
            "Epoch 10/15\n",
            "782/782 - 211s - loss: 0.0190 - accuracy: 0.9945 - val_loss: 1.0421 - val_accuracy: 0.8167 - 211s/epoch - 269ms/step\n",
            "Epoch 11/15\n",
            "782/782 - 212s - loss: 0.0186 - accuracy: 0.9941 - val_loss: 1.0290 - val_accuracy: 0.8083 - 212s/epoch - 271ms/step\n",
            "Epoch 12/15\n",
            "782/782 - 209s - loss: 0.0156 - accuracy: 0.9953 - val_loss: 0.9681 - val_accuracy: 0.8060 - 209s/epoch - 267ms/step\n",
            "Epoch 13/15\n",
            "782/782 - 208s - loss: 0.0128 - accuracy: 0.9958 - val_loss: 1.0094 - val_accuracy: 0.8081 - 208s/epoch - 266ms/step\n",
            "Epoch 14/15\n",
            "782/782 - 210s - loss: 0.0084 - accuracy: 0.9974 - val_loss: 1.3653 - val_accuracy: 0.8094 - 210s/epoch - 268ms/step\n",
            "Epoch 15/15\n",
            "782/782 - 208s - loss: 0.0090 - accuracy: 0.9971 - val_loss: 1.0490 - val_accuracy: 0.8096 - 208s/epoch - 266ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b9d285e5ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, let's evaluate our model's accuracy:"
      ],
      "metadata": {
        "id": "Kqoj5vjMifhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save(\"my_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc7ePv-yx6mk",
        "outputId": "113ab4a8-c8af-4757-fd89-b61a72b5ef0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = load_model('my_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVEaeyE2z_g8",
        "outputId": "fe43f423-0a63-44d3-badc-4f279e7f6af5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=32,\n",
        "                            verbose=2)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_N-S3LLifhP",
        "outputId": "4484dfa4-be25-4cf3-ee7e-14c7c39756f2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 - 18s - loss: 1.0490 - accuracy: 0.8096 - 18s/epoch - 23ms/step\n",
            "Test score: 1.0490118265151978\n",
            "Test accuracy: 0.8096399903297424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Achieving an accuracy of 81% is quite commendable, especially considering the limitation to just the first 80 words of each review. However, it's noteworthy that the validation accuracy plateaued early in training, indicating overfitting. Employing early stopping could have mitigated this issue effectively.\n",
        "\n",
        "Nevertheless, the accomplishment is significant. We've developed a neural network capable of discerning the sentiment of movie reviews based solely on text. The model comprehends the context and positioning of words within each review, showcasing the power of Keras. Remarkably, setting up the model required only a few lines of code. It's a testament to the remarkable capabilities afforded by modern machine learning frameworks.\n"
      ],
      "metadata": {
        "id": "7rTaa7rxifhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[0],len(x_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "987MvT0IxLwa",
        "outputId": "127e99d7-ed98-4804-b40a-0baf1157b199"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     1,   591,   202,    14,    31,     6,\n",
              "          717,    10,    10, 18142, 10698,     5,     4,   360,     7,\n",
              "            4,   177,  5760,   394,   354,     4,   123,     9,  1035,\n",
              "         1035,  1035,    10,    10,    13,    92,   124,    89,   488,\n",
              "         7944,   100,    28,  1668,    14,    31,    23,    27,  7479,\n",
              "           29,   220,   468,     8,   124,    14,   286,   170,     8,\n",
              "          157,    46,     5,    27,   239,    16,   179, 15387,    38,\n",
              "           32,    25,  7944,   451,   202,    14,     6,   717],\n",
              "       dtype=int32),\n",
              " 80)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decode_review(review):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in review])\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "    # Decode the original review from the tokenized format to words\n",
        "    original_review = decode_review(x_test[i])\n",
        "\n",
        "    # Get the corresponding ground truth label\n",
        "    ground_truth_label = y_test[i]\n",
        "\n",
        "    # Predict using the model\n",
        "    predictions = model.predict(np.expand_dims(x_test[i], axis=0))\n",
        "\n",
        "    # Assuming your model outputs probabilities for each class\n",
        "    predicted_label = np.argmax(predictions)\n",
        "\n",
        "    print('\\n\\nExample', i+1)\n",
        "    print('Original Review:', original_review)\n",
        "    print('Ground Truth Label:', ground_truth_label)\n",
        "    print('Predicted Label:', predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NHAwHGFw9zm",
        "outputId": "821526f6-fdfb-43a3-9953-c0f2ae64ce32"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 231ms/step\n",
            "\n",
            "\n",
            "Example 1\n",
            "Original Review: ? ? ? ? ? ? ? ? ? ? ? ? ? please give this one a miss br br kristy swanson and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite lacklustre so all you madison fans give this a miss\n",
            "Ground Truth Label: 0\n",
            "Predicted Label: 0\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "\n",
            "\n",
            "Example 2\n",
            "Original Review: wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to elicit a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n",
            "Ground Truth Label: 1\n",
            "Predicted Label: 0\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "\n",
            "\n",
            "Example 3\n",
            "Original Review: events ? may or may not have had ? turmoil in mind when he made ? but whatever prompted his choice of material the film stands as a cautionary tale of universal application ? could be the soviet union italy germany or japan in the 1930s or any country of any era that lets its guard down and is overwhelmed by tyranny it's a fascinating film even a charming one in its macabre way but its message is no joke\n",
            "Ground Truth Label: 1\n",
            "Predicted Label: 0\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "\n",
            "\n",
            "Example 4\n",
            "Original Review: angry about her and her actions because i would have otherwise enjoyed the flick what a number she was take my advise and fast forward through everything you see her do until the end also is anyone else getting sick of watching movies that are filmed so dark anymore one can hardly see what is being filmed as an audience we are impossibly involved with the actions on the screen so then why the hell can't we have night vision\n",
            "Ground Truth Label: 0\n",
            "Predicted Label: 0\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "\n",
            "\n",
            "Example 5\n",
            "Original Review: worth it as soon as you start your hooked the levels are fun and exiting they will hook you ? your mind turns to mush i'm not kidding this game is also orchestrated and is beautifully done br br to keep this spoiler free i have to keep my mouth shut about details but please try this game it'll be worth it br br story 9 9 action 10 1 it's that good ? 10 attention ? 10 average 10\n",
            "Ground Truth Label: 1\n",
            "Predicted Label: 0\n"
          ]
        }
      ]
    }
  ]
}